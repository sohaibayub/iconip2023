{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1686075032319,
     "user": {
      "displayName": "Faizad Ullah",
      "userId": "13688105782386748277"
     },
     "user_tz": -300
    },
    "id": "ZKH9ekMIX1BY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ali.faheem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from subprocess import check_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import locale\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "os.environ[\"PYTHONIOENCODING\"] = \"utf-8\"\n",
    "# myLocale=locale.setlocale(category=locale.LC_ALL, locale=\"en_GB.UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1202,
     "status": "ok",
     "timestamp": 1686075208283,
     "user": {
      "displayName": "Faizad Ullah",
      "userId": "13688105782386748277"
     },
     "user_tz": -300
    },
    "id": "8ivCayD2Z0tO"
   },
   "outputs": [],
   "source": [
    "path = os.path.join(\".\", r\"D:\\Faizad\\Intelligent Computing Project\\Results\\RUHSOLD\\dataset_old_non_repeat_combined.txt\")\n",
    "df = pd.read_csv(path, sep='\\t') # trying to make the first row index_col=0\n",
    "df.dropna(inplace=True)\n",
    "combined_dataset = df[['Tweet','Score', 'Tag']]\n",
    "\n",
    "# combined_dataset['label'] = combined_dataset['label'].replace([0],'normal').replace([1],'chutiya bc')\n",
    "\n",
    "combined_dataset['Score'] = combined_dataset['Score'].replace([5,4],'overtly_hateful')\n",
    "combined_dataset['Score'] = combined_dataset['Score'].replace([3,2,1],'hateful')\n",
    "combined_dataset['Score'] = combined_dataset['Score'].replace([0],'no_hate')\n",
    "combined_dataset['Tweet'] = combined_dataset['Tweet'].replace([0],'no_hate')\n",
    "\n",
    "combined_dataset\n",
    "data_frame = combined_dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Score</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chal hamare to shabdo ke mtlb ko badal k &lt;Rel&gt;...</td>\n",
       "      <td>overtly_hateful</td>\n",
       "      <td>Religious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;A/O&gt; bhen chod Main sirf kotha khulna baki re...</td>\n",
       "      <td>overtly_hateful</td>\n",
       "      <td>Abusive/Offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;A/O&gt; Bhen ki lodi &lt;A/O&gt; . Bigg boss nahi dekh...</td>\n",
       "      <td>overtly_hateful</td>\n",
       "      <td>Abusive/Offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;U&gt; Bhen chod &lt;U&gt; twitter ko twitter kam &lt;U&gt; k...</td>\n",
       "      <td>hateful</td>\n",
       "      <td>Untargeted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;U&gt; Bhen chod lun &lt;U&gt; ko tou mxakh smjh rakha ...</td>\n",
       "      <td>overtly_hateful</td>\n",
       "      <td>Untargeted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;A/O&gt; Mai to maa chod du uski &lt;A/O&gt; jo meri ma...</td>\n",
       "      <td>overtly_hateful</td>\n",
       "      <td>Abusive/Offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Main v Changa, Mere phone v changa, COD v chan...</td>\n",
       "      <td>hateful</td>\n",
       "      <td>Abusive/Offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Naahi us ki bhen ka yaar naahi magar itna pata...</td>\n",
       "      <td>overtly_hateful</td>\n",
       "      <td>Abusive/Offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ye sanha ab bht over kr rhi ha bhen tu jitna m...</td>\n",
       "      <td>no_hate</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;A/O&gt; Bhen k lund &lt;A/O&gt; &lt;A/O&gt; ye vahi hai jisn...</td>\n",
       "      <td>overtly_hateful</td>\n",
       "      <td>Religious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Es &lt;A/O&gt; bhen chod ka kam rundian &lt;A/O&gt; ko sel...</td>\n",
       "      <td>overtly_hateful</td>\n",
       "      <td>Abusive/Offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ye voda wale &lt;A/O&gt; chutiya &lt;A/O&gt; hai. &lt;A/O&gt; Bh...</td>\n",
       "      <td>overtly_hateful</td>\n",
       "      <td>Abusive/Offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bibi hosh karo qued e azam aur allama iqbal Al...</td>\n",
       "      <td>no_hate</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>#HareemShahScandal Sir please do something. Ha...</td>\n",
       "      <td>no_hate</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bandr marna nai &lt;U&gt; bc &lt;U&gt; bandr killa ha ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚</td>\n",
       "      <td>hateful</td>\n",
       "      <td>Untargeted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bandr killa ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚</td>\n",
       "      <td>no_hate</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Imran khan :sakoon sirf qabr main milta hai &lt;A...</td>\n",
       "      <td>hateful</td>\n",
       "      <td>Abusive/Offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>&lt;A/O&gt; Bandr &lt;A/O&gt; ke padne se kch nahi hota</td>\n",
       "      <td>hateful</td>\n",
       "      <td>Abusive/Offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Mahira toh .real...ki... &lt;A/O&gt; bandr...lgti......</td>\n",
       "      <td>hateful</td>\n",
       "      <td>Abusive/Offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Tum sports anchor sa &lt;A/O&gt; syasi bandr &lt;A/O&gt; k...</td>\n",
       "      <td>hateful</td>\n",
       "      <td>Abusive/Offensive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tweet            Score  \\\n",
       "0   Chal hamare to shabdo ke mtlb ko badal k <Rel>...  overtly_hateful   \n",
       "1   <A/O> bhen chod Main sirf kotha khulna baki re...  overtly_hateful   \n",
       "2   <A/O> Bhen ki lodi <A/O> . Bigg boss nahi dekh...  overtly_hateful   \n",
       "3   <U> Bhen chod <U> twitter ko twitter kam <U> k...          hateful   \n",
       "4   <U> Bhen chod lun <U> ko tou mxakh smjh rakha ...  overtly_hateful   \n",
       "5   <A/O> Mai to maa chod du uski <A/O> jo meri ma...  overtly_hateful   \n",
       "6   Main v Changa, Mere phone v changa, COD v chan...          hateful   \n",
       "7   Naahi us ki bhen ka yaar naahi magar itna pata...  overtly_hateful   \n",
       "8   Ye sanha ab bht over kr rhi ha bhen tu jitna m...          no_hate   \n",
       "9   <A/O> Bhen k lund <A/O> <A/O> ye vahi hai jisn...  overtly_hateful   \n",
       "10  Es <A/O> bhen chod ka kam rundian <A/O> ko sel...  overtly_hateful   \n",
       "11  Ye voda wale <A/O> chutiya <A/O> hai. <A/O> Bh...  overtly_hateful   \n",
       "12  Bibi hosh karo qued e azam aur allama iqbal Al...          no_hate   \n",
       "13  #HareemShahScandal Sir please do something. Ha...          no_hate   \n",
       "14    Bandr marna nai <U> bc <U> bandr killa ha ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚          hateful   \n",
       "15                                  Bandr killa ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚          no_hate   \n",
       "16  Imran khan :sakoon sirf qabr main milta hai <A...          hateful   \n",
       "17        <A/O> Bandr <A/O> ke padne se kch nahi hota          hateful   \n",
       "18  Mahira toh .real...ki... <A/O> bandr...lgti......          hateful   \n",
       "19  Tum sports anchor sa <A/O> syasi bandr <A/O> k...          hateful   \n",
       "\n",
       "                  Tag  \n",
       "0           Religious  \n",
       "1   Abusive/Offensive  \n",
       "2   Abusive/Offensive  \n",
       "3          Untargeted  \n",
       "4          Untargeted  \n",
       "5   Abusive/Offensive  \n",
       "6   Abusive/Offensive  \n",
       "7   Abusive/Offensive  \n",
       "8             Neutral  \n",
       "9           Religious  \n",
       "10  Abusive/Offensive  \n",
       "11  Abusive/Offensive  \n",
       "12            Neutral  \n",
       "13            Neutral  \n",
       "14         Untargeted  \n",
       "15            Neutral  \n",
       "16  Abusive/Offensive  \n",
       "17  Abusive/Offensive  \n",
       "18  Abusive/Offensive  \n",
       "19  Abusive/Offensive  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1686078169723,
     "user": {
      "displayName": "Faizad Ullah",
      "userId": "13688105782386748277"
     },
     "user_tz": -300
    },
    "id": "MAfuZqChdONU"
   },
   "outputs": [],
   "source": [
    "# path1 = os.path.join(\".\", \"D:/Faizad/Intelligent Computing Project/Python_Notebooks/RUHSOLD_Dataset/New folder/Hate_Speech.txt\")\n",
    "# hate_speech_df = pd.read_csv(path1, header=None) # trying to make the first row index_col=0\n",
    "# # hate_speech_df\n",
    "# hate_speech_list = list(hate_speech_df[0])\n",
    "# hate_speech = ' '.join(str(item) for item in hate_speech_list)\n",
    "\n",
    "# path2 = os.path.join(\".\", \"D:/Faizad/Intelligent Computing Project/Python_Notebooks/RUHSOLD_Dataset/New folder/Offensive.txt\")\n",
    "# Offensive_df = pd.read_csv(path2, header=None) # trying to make the first row index_col=0\n",
    "# Offensive_list = list(Offensive_df[0])\n",
    "# Offensive = ' '.join(str(item) for item in Offensive_list)\n",
    "\n",
    "# path3 = os.path.join(\".\", \"D:/Faizad/Intelligent Computing Project/Python_Notebooks/RUHSOLD_Dataset/New folder/Harassment .txt\")\n",
    "# Harassment_df = pd.read_csv(path3, header=None) # trying to make the first row index_col=0\n",
    "# Harassment_list = list(Harassment_df[0])\n",
    "# Harassment = ' '.join(str(item) for item in Harassment_list)\n",
    "\n",
    "# path4 = os.path.join(\".\", \"D:/Faizad/Intelligent Computing Project/Python_Notebooks/RUHSOLD_Dataset/New folder/Normal.txt\")\n",
    "# Normal_df = pd.read_csv(path4, header=None) # trying to make the first row index_col=0\n",
    "# Normal_list = list(Normal_df[0])\n",
    "# Normal = ' '.join(str(item) for item in Normal_list)\n",
    "\n",
    "# path5 = os.path.join(\".\", \"D:/Faizad/Intelligent Computing Project/Python_Notebooks/RUHSOLD_Dataset/New folder/Defamation.txt\")\n",
    "# Defamation_df = pd.read_csv(path5, header=None) # trying to make the first row index_col=0\n",
    "# Defamation_list = list(Defamation_df[0])\n",
    "# Defamation = ' '.join(str(item) for item in Defamation_list)\n",
    "\n",
    "# path6 = os.path.join(\".\", \"D:/Faizad/Intelligent Computing Project/Python_Notebooks/RUHSOLD_Dataset/New folder/Terrorism.txt\")\n",
    "# Terrorism_df = pd.read_csv(path6, header=None) # trying to make the first row index_col=0\n",
    "# Terrorism_list = list(Terrorism_df[0])\n",
    "# Terrorism = ' '.join(str(item) for item in Terrorism_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "    df['fuzz_ratio_hate_speech'] = df.apply(lambda x: fuzz.ratio(str(x['Tweet']), str(x['Tag'])), axis=1)\n",
    "    df['fuzz_ratio_hate_speech'] = df['fuzz_ratio_hate_speech']/100\n",
    "    \n",
    "    df['fuzz_qratio_Offensive'] = df.apply(lambda x: fuzz.QRatio(str(x['Tweet']), str(x['Tag'])), axis=1)\n",
    "    df['fuzz_qratio_Offensive'] = df['fuzz_qratio_Offensive'] / 100\n",
    "    \n",
    "    df['fuzz_uqratio_hate_speech'] = df.apply(lambda x: fuzz.UQRatio(str(x['Tweet']), str(x['Tag'])), axis=1)\n",
    "    df['fuzz_uqratio_hate_speech'] = df['fuzz_uqratio_hate_speech'] / 100\n",
    "    \n",
    "    df['fuzz_uwratio_hate_speech'] = df.apply(lambda x: fuzz.UWRatio(str(x['Tweet']), str(x['Tag'])), axis=1) \n",
    "    df['fuzz_uwratio_hate_speech'] = df['fuzz_uwratio_hate_speech'] / 100\n",
    "    \n",
    "    df['fuzz_wratio_hate_speech'] = df.apply(lambda x: fuzz.WRatio(str(x['Tweet']), str(x['Tag'])), axis=1)\n",
    "    df['fuzz_wratio_hate_speech'] = df['fuzz_wratio_hate_speech'] / 100\n",
    "    \n",
    "    df['fuzz_partial_ratio_hate_speech'] = df.apply(lambda x: fuzz.partial_ratio(str(x['Tweet']), str(x['Tag'])), axis=1)\n",
    "    df['fuzz_partial_ratio_hate_speech'] = df['fuzz_partial_ratio_hate_speech'] / 100\n",
    "    \n",
    "    df['fuzz_partial_token_set_ratio_hate_speech'] = df.apply(lambda x: fuzz.partial_token_set_ratio(str(x['Tag']), str(x['Tag'])), axis=1)\n",
    "    df['fuzz_partial_token_set_ratio_hate_speech'] = df['fuzz_partial_token_set_ratio_hate_speech'] / 100\n",
    "    \n",
    "    df['fuzz_partial_token_sort_ratio_hate_speech'] = df.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['Tweet']), str(x['Tag'])), axis=1)\n",
    "    df['fuzz_partial_token_sort_ratio_hate_speech'] = df['fuzz_partial_token_sort_ratio_hate_speech'] / 100\n",
    "    \n",
    "    df['fuzz_token_set_ratio_hate_speech'] = df.apply(lambda x: fuzz.token_set_ratio(str(x['Tweet']), str(x['Tag'])), axis=1)\n",
    "    df['fuzz_token_set_ratio_hate_speech'] = df['fuzz_token_set_ratio_hate_speech'] / 100\n",
    "    \n",
    "    df['fuzz_token_sort_ratio_hate_speech'] = df.apply(lambda x: fuzz.token_sort_ratio(str(x['Tweet']), str(x['Tag'])), axis=1)\n",
    "    df['fuzz_token_sort_ratio_hate_speech'] = df['fuzz_token_sort_ratio_hate_speech'] / 100\n",
    "    \n",
    "    return df\n",
    "df = extract_features(data_frame)\n",
    "# print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1686079622777,
     "user": {
      "displayName": "Faizad Ullah",
      "userId": "13688105782386748277"
     },
     "user_tz": -300
    },
    "id": "TWzwQpKfbE_8",
    "outputId": "1595b7eb-5712-437e-aa18-e11f16bcb6cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create an instance of the TfidfVectorizer class\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# fit the vectorizer to the documents and transform the documents into TF-IDF format\n",
    "tfidf_matrix = vectorizer.fit_transform(df['Tweet'])\n",
    "\n",
    "# print the resulting TF-IDF matrix\n",
    "print(tfidf_matrix[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BPhdgcYcrI0Y"
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# # Create an instance of the CountVectorizer class\n",
    "# vectorizer1 = CountVectorizer()\n",
    "\n",
    "# # Fit the vectorizer to the documents and transform the documents into BoW format\n",
    "# bow_matrix = vectorizer1.fit_transform(df['Tweet'])\n",
    "\n",
    "# # Print the resulting BoW matrix\n",
    "# print(bow_matrix[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fOnYyUoLwD1i"
   },
   "outputs": [],
   "source": [
    "# # Calculate the average word length for each tweet\n",
    "# avg_word_length = df['Tweet'].apply(lambda x: np.mean([len(word) for word in x.split()]))\n",
    "\n",
    "# # Convert average word length to a numpy array and reshape it as a column vector\n",
    "# avg_word_length_vector = np.array(avg_word_length).reshape(-1, 1)\n",
    "\n",
    "# # Print the average word length vector\n",
    "# print(len(avg_word_length_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "nKH9aBPYw0at"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Calculate the character count for each tweet\n",
    "# char_count = df['Tweet'].apply(lambda x: len(x))\n",
    "\n",
    "# # Convert character count to a numpy array and reshape it as a column vector\n",
    "# char_count_vector = np.array(char_count).reshape(-1, 1)\n",
    "\n",
    "# # Print the character count vector\n",
    "# print((char_count_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 622,
     "status": "ok",
     "timestamp": 1686079630195,
     "user": {
      "displayName": "Faizad Ullah",
      "userId": "13688105782386748277"
     },
     "user_tz": -300
    },
    "id": "pBDJYTD0QDuW"
   },
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray())\n",
    "\n",
    "# bow_df = pd.DataFrame(bow_matrix.toarray())\n",
    "\n",
    "# # awl_df = pd.DataFrame(avg_word_length_vector)\n",
    "\n",
    "# # ccv_df = pd.DataFrame(char_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1686079672012,
     "user": {
      "displayName": "Faizad Ullah",
      "userId": "13688105782386748277"
     },
     "user_tz": -300
    },
    "id": "wJy5LjPFb0CB"
   },
   "outputs": [],
   "source": [
    "feature_columns = df.columns.drop(['Tweet','Score','Tag'])\n",
    "df1 = pd.DataFrame()\n",
    "df1 = df[feature_columns]\n",
    "# df1\n",
    "\n",
    "# feature_columns = df.columns.drop(['sentence','label'])\n",
    "# df1 = pd.DataFrame()\n",
    "# df1 = df[feature_columns]\n",
    "# # df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 6839,
     "status": "ok",
     "timestamp": 1686079686255,
     "user": {
      "displayName": "Faizad Ullah",
      "userId": "13688105782386748277"
     },
     "user_tz": -300
    },
    "id": "tCi4USYHb9IX"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fuzz_ratio_hate_speech</th>\n",
       "      <th>fuzz_qratio_Offensive</th>\n",
       "      <th>fuzz_uqratio_hate_speech</th>\n",
       "      <th>fuzz_uwratio_hate_speech</th>\n",
       "      <th>fuzz_wratio_hate_speech</th>\n",
       "      <th>fuzz_partial_ratio_hate_speech</th>\n",
       "      <th>fuzz_partial_token_set_ratio_hate_speech</th>\n",
       "      <th>fuzz_partial_token_sort_ratio_hate_speech</th>\n",
       "      <th>fuzz_token_set_ratio_hate_speech</th>\n",
       "      <th>fuzz_token_sort_ratio_hate_speech</th>\n",
       "      <th>...</th>\n",
       "      <th>22308</th>\n",
       "      <th>22309</th>\n",
       "      <th>22310</th>\n",
       "      <th>22311</th>\n",
       "      <th>22312</th>\n",
       "      <th>22313</th>\n",
       "      <th>22314</th>\n",
       "      <th>22315</th>\n",
       "      <th>22316</th>\n",
       "      <th>22317</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10006</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10007</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10008 rows Ã— 22328 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       fuzz_ratio_hate_speech  fuzz_qratio_Offensive  \\\n",
       "0                        0.05                   0.05   \n",
       "1                        0.14                   0.14   \n",
       "2                        0.10                   0.11   \n",
       "3                        0.16                   0.17   \n",
       "4                        0.15                   0.16   \n",
       "...                       ...                    ...   \n",
       "10003                    0.15                   0.16   \n",
       "10004                    0.06                   0.07   \n",
       "10005                    0.10                   0.13   \n",
       "10006                    0.10                   0.11   \n",
       "10007                    0.13                   0.13   \n",
       "\n",
       "       fuzz_uqratio_hate_speech  fuzz_uwratio_hate_speech  \\\n",
       "0                          0.05                      0.26   \n",
       "1                          0.14                      0.23   \n",
       "2                          0.11                      0.21   \n",
       "3                          0.17                      0.27   \n",
       "4                          0.16                      0.34   \n",
       "...                         ...                       ...   \n",
       "10003                      0.16                      0.36   \n",
       "10004                      0.07                      0.26   \n",
       "10005                      0.13                      0.26   \n",
       "10006                      0.11                      0.26   \n",
       "10007                      0.13                      0.26   \n",
       "\n",
       "       fuzz_wratio_hate_speech  fuzz_partial_ratio_hate_speech  \\\n",
       "0                         0.26                            0.44   \n",
       "1                         0.23                            0.29   \n",
       "2                         0.21                            0.35   \n",
       "3                         0.27                            0.30   \n",
       "4                         0.34                            0.20   \n",
       "...                        ...                             ...   \n",
       "10003                     0.36                            0.40   \n",
       "10004                     0.26                            0.43   \n",
       "10005                     0.26                            0.43   \n",
       "10006                     0.26                            0.29   \n",
       "10007                     0.26                            0.43   \n",
       "\n",
       "       fuzz_partial_token_set_ratio_hate_speech  \\\n",
       "0                                           1.0   \n",
       "1                                           1.0   \n",
       "2                                           1.0   \n",
       "3                                           1.0   \n",
       "4                                           1.0   \n",
       "...                                         ...   \n",
       "10003                                       1.0   \n",
       "10004                                       1.0   \n",
       "10005                                       1.0   \n",
       "10006                                       1.0   \n",
       "10007                                       1.0   \n",
       "\n",
       "       fuzz_partial_token_sort_ratio_hate_speech  \\\n",
       "0                                           0.44   \n",
       "1                                           0.41   \n",
       "2                                           0.35   \n",
       "3                                           0.30   \n",
       "4                                           0.40   \n",
       "...                                          ...   \n",
       "10003                                       0.40   \n",
       "10004                                       0.43   \n",
       "10005                                       0.43   \n",
       "10006                                       0.29   \n",
       "10007                                       0.29   \n",
       "\n",
       "       fuzz_token_set_ratio_hate_speech  fuzz_token_sort_ratio_hate_speech  \\\n",
       "0                                  0.06                               0.05   \n",
       "1                                  0.19                               0.17   \n",
       "2                                  0.17                               0.14   \n",
       "3                                  0.19                               0.18   \n",
       "4                                  0.17                               0.15   \n",
       "...                                 ...                                ...   \n",
       "10003                              0.14                               0.14   \n",
       "10004                              0.07                               0.07   \n",
       "10005                              0.11                               0.10   \n",
       "10006                              0.09                               0.08   \n",
       "10007                              0.09                               0.09   \n",
       "\n",
       "       ...  22308  22309  22310  22311  22312  22313  22314  22315  22316  \\\n",
       "0      ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1      ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "2      ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3      ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4      ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "10003  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "10004  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "10005  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "10006  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "10007  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "       22317  \n",
       "0        0.0  \n",
       "1        0.0  \n",
       "2        0.0  \n",
       "3        0.0  \n",
       "4        0.0  \n",
       "...      ...  \n",
       "10003    0.0  \n",
       "10004    0.0  \n",
       "10005    0.0  \n",
       "10006    0.0  \n",
       "10007    0.0  \n",
       "\n",
       "[10008 rows x 22328 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df = pd.concat([df1, tfidf_df], axis=1)\n",
    "concatenated_df\n",
    "# concatenated_df = pd.concat([tfidf_df, bow_df, awl_df, ccv_df,  df1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 9525,
     "status": "ok",
     "timestamp": 1686079712396,
     "user": {
      "displayName": "Faizad Ullah",
      "userId": "13688105782386748277"
     },
     "user_tz": -300
    },
    "id": "4H0SS0TCX4KQ"
   },
   "outputs": [],
   "source": [
    "# feature_columns = df.columns.drop(['Score','Tag'])\n",
    "\n",
    "X_normalized = normalize(concatenated_df, norm='l2', axis=1, copy=True, return_norm=False)\n",
    "# print(X_normalized)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_normalized, df['Tag'], random_state=1, test_size=0.2)\n",
    "\n",
    "result_cols = [\"Classifier\", \"Accuracy\"]\n",
    "result_frame = pd.DataFrame(columns=result_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1686079712396,
     "user": {
      "displayName": "Faizad Ullah",
      "userId": "13688105782386748277"
     },
     "user_tz": -300
    },
    "id": "2jeGj9uOX4SO"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classifiers = [\n",
    "#     DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "#     LogisticRegression(),\n",
    "    KNeighborsClassifier(),\n",
    "#     AdaBoostClassifier(),\n",
    "#     GaussianNB(),\n",
    "#     SVC(gamma='auto'),\n",
    "    ]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Abusive/Offensive       0.93      0.81      0.87       497\n",
      "          Neutral       0.86      0.99      0.92      1081\n",
      "        Religious       0.98      0.92      0.95       165\n",
      "           Sexist       0.94      0.75      0.84       159\n",
      "       Untargeted       0.93      0.43      0.59       100\n",
      "\n",
      "         accuracy                           0.89      2002\n",
      "        macro avg       0.93      0.78      0.83      2002\n",
      "     weighted avg       0.90      0.89      0.89      2002\n",
      "\n",
      "RandomForestClassifier accuracy = 89.21078921078922%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ali.faheem\\AppData\\Local\\Temp\\ipykernel_2180\\775732946.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_frame = result_frame.append(acc_field)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Abusive/Offensive       0.71      0.59      0.64       497\n",
      "          Neutral       0.77      0.90      0.83      1081\n",
      "        Religious       0.81      0.87      0.84       165\n",
      "           Sexist       0.55      0.49      0.52       159\n",
      "       Untargeted       0.48      0.10      0.17       100\n",
      "\n",
      "         accuracy                           0.74      2002\n",
      "        macro avg       0.66      0.59      0.60      2002\n",
      "     weighted avg       0.73      0.74      0.73      2002\n",
      "\n",
      "KNeighborsClassifier accuracy = 74.47552447552448%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ali.faheem\\AppData\\Local\\Temp\\ipykernel_2180\\775732946.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_frame = result_frame.append(acc_field)\n"
     ]
    }
   ],
   "source": [
    "for clf in classifiers:\n",
    "    name = clf.__class__.__name__\n",
    "    clf.fit(x_train, y_train)\n",
    "    predicted = clf.predict(x_test)\n",
    "    print(classification_report(y_test,predicted))\n",
    "#     classification_rep = classification_report(y_test,predicted)\n",
    "#     output_folder = \"D:/Faizad/Intelligent Computing Project/Results/RUHSOLD/TF-IDF\"\n",
    "#     output_filename = name+('.txt')\n",
    "#     output_path = os.path.join(output_folder, output_filename)\n",
    "#     with open(output_path, \"w\", encoding='utf-8') as f:\n",
    "#         f.write(classification_rep)\n",
    "    acc = metrics.accuracy_score(y_test,predicted)\n",
    "    print (name +' accuracy = '+str(acc*100)+'%')\n",
    "    acc_field = pd.DataFrame([[name, acc*100]], columns=result_cols)\n",
    "    result_frame = result_frame.append(acc_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_features(df):\n",
    "# #     df = []\n",
    "# #     df['fuzz_qratio_hate_speech'] = df.apply(lambda x: fuzz.QRatio(str(x['Tweet']), str(hate_speech+Offensive+Harassment+Normal+Defamation+Terrorism)), axis=1)\n",
    "#     df['fuzz_qratio_hate_speech'] = df.apply(lambda x: fuzz.ratio(str(x['Tweet']), str(x['Tag'])), axis=1)\n",
    "#     df['fuzz_qratio_hate_speech'] = df['fuzz_qratio_hate_speech']/100\n",
    "    \n",
    "#     df['fuzz_qratio_Offensive'] = df.apply(lambda x: fuzz.ratio(str(x['Tweet']), str(Offensive)), axis=1)\n",
    "#     df['fuzz_qratio_Offensive'] = df['fuzz_qratio_Offensive']/100\n",
    "    \n",
    "#     df['fuzz_qratio_Harassment'] = df.apply(lambda x: fuzz.ratio(str(x['Tweet']), str(Harassment)), axis=1)\n",
    "#     df['fuzz_qratio_Harassment'] = df['fuzz_qratio_Harassment'] /100\n",
    "    \n",
    "#     df['fuzz_qratio_Normal'] = df.apply(lambda x: fuzz.ratio(str(x['Tweet']), str(Normal)), axis=1)\n",
    "#     df['fuzz_qratio_Normal'] = df['fuzz_qratio_Normal'] /100\n",
    "    \n",
    "#     df['fuzz_qratio_Defamation'] = df.apply(lambda x: fuzz.ratio(str(x['Tweet']), str(Defamation)), axis=1)\n",
    "#     df['fuzz_qratio_Defamation'] = df['fuzz_qratio_Defamation'] /100\n",
    "    \n",
    "#     df['fuzz_qratio_Terrorism'] = df.apply(lambda x: fuzz.ratio(str(x['Tweet']), str(Terrorism)), axis=1)\n",
    "#     df['fuzz_qratio_Terrorism'] = df['fuzz_qratio_Terrorism'] /100\n",
    "    \n",
    "# #     df['fuzz_uqratio_hate_speech'] = df.apply(lambda x: fuzz.UQRatio(str(x['Tweet']), str(hate_speech)), axis=1)\n",
    "#     df['fuzz_uqratio_Offensive'] = df.apply(lambda x: fuzz.UQRatio(str(x['Tweet']), str(Offensive)), axis=1)\n",
    "# #     df['fuzz_uqratio_Harassment'] = df.apply(lambda x: fuzz.UQRatio(str(x['Tweet']), str(Harassment)), axis=1)\n",
    "# #     df['fuzz_uqratio_Normal'] = df.apply(lambda x: fuzz.UQRatio(str(x['Tweet']), str(Normal)), axis=1)\n",
    "# #     df['fuzz_uqratio_Defamation'] = df.apply(lambda x: fuzz.UQRatio(str(x['Tweet']), str(Defamation)), axis=1)\n",
    "# #     df['fuzz_uqratio_Terrorism'] = df.apply(lambda x: fuzz.UQRatio(str(x['Tweet']), str(Terrorism)), axis=1)\n",
    "    \n",
    "# #     df['fuzz_uwratio_hate_speech'] = df.apply(lambda x: fuzz.UWRatio(str(x['Tweet']), str(hate_speech)), axis=1)\n",
    "# #     df['fuzz_uwratio_Offensive'] = df.apply(lambda x: fuzz.UWRatio(str(x['Tweet']), str(Offensive)), axis=1)\n",
    "#     df['fuzz_uwratio_Harassment'] = df.apply(lambda x: fuzz.UWRatio(str(x['Tweet']), str(Harassment)), axis=1)\n",
    "# #     df['fuzz_uwratio_Normal'] = df.apply(lambda x: fuzz.UWRatio(str(x['Tweet']), str(Normal)), axis=1)\n",
    "# #     df['fuzz_uwratio_Defamation'] = df.apply(lambda x: fuzz.UWRatio(str(x['Tweet']), str(Defamation)), axis=1)\n",
    "# #     df['fuzz_uwratio_Terrorism'] = df.apply(lambda x: fuzz.UWRatio(str(x['Tweet']), str(Terrorism)), axis=1)\n",
    "    \n",
    "# #     df['fuzz_wratio_hate_speech'] = df.apply(lambda x: fuzz.WRatio(str(x['Tweet']), str(hate_speech)), axis=1)\n",
    "# #     df['fuzz_wratio_Offensive'] = df.apply(lambda x: fuzz.WRatio(str(x['Tweet']), str(Offensive)), axis=1)\n",
    "# #     df['fuzz_wratio_Harassment'] = df.apply(lambda x: fuzz.WRatio(str(x['Tweet']), str(Harassment)), axis=1)\n",
    "#     df['fuzz_wratio_Normal'] = df.apply(lambda x: fuzz.WRatio(str(x['Tweet']), str(Normal)), axis=1)\n",
    "# #     df['fuzz_wratio_Defamation'] = df.apply(lambda x: fuzz.WRatio(str(x['Tweet']), str(Defamation)), axis=1)\n",
    "# #     df['fuzz_wratio_Terrorism'] = df.apply(lambda x: fuzz.WRatio(str(x['Tweet']), str(Terrorism)), axis=1)\n",
    "    \n",
    "#     df['fuzz_partial_ratio_hate_speech'] = df.apply(lambda x: fuzz.partial_ratio(str(x['Tweet']), str(x['Tag'])), axis=1)\n",
    "#     df['fuzz_partial_ratio_hate_speech'] = df['fuzz_partial_ratio_hate_speech'] / 100\n",
    "# # #     df['fuzz_partial_ratio_Offensive'] = df.apply(lambda x: fuzz.partial_ratio(str(x['Tweet']), str(Offensive)), axis=1)\n",
    "# # #     df['fuzz_partial_ratio_Harassment'] = df.apply(lambda x: fuzz.partial_ratio(str(x['Tweet']), str(Harassment)), axis=1)\n",
    "# # #     df['fuzz_partial_ratio_Normal'] = df.apply(lambda x: fuzz.partial_ratio(str(x['Tweet']), str(Normal)), axis=1)\n",
    "# #     df['fuzz_partial_ratio_Defamation'] = df.apply(lambda x: fuzz.partial_ratio(str(x['Tweet']), str(Defamation)), axis=1)\n",
    "# # #     df['fuzz_partial_ratio_Terrorism'] = df.apply(lambda x: fuzz.partial_ratio(str(x['Tweet']), str(Terrorism)), axis=1)\n",
    "    \n",
    "#     df['fuzz_partial_token_set_ratio_hate_speech'] = df.apply(lambda x: fuzz.partial_token_set_ratio(str(x['Tweet']), str(x['Tag'])), axis=1)\n",
    "#     df['fuzz_partial_token_set_ratio_hate_speech'] = df['fuzz_partial_token_set_ratio_hate_speech'] / 100\n",
    "# #     df['fuzz_partial_token_set_ratio_Offensive'] = df.apply(lambda x: fuzz.partial_token_set_ratio(str(x['Tweet']), str(Offensive)), axis=1)\n",
    "# #     df['fuzz_partial_token_set_ratio_Harassment'] = df.apply(lambda x: fuzz.partial_token_set_ratio(str(x['Tweet']), str(Harassment)), axis=1)\n",
    "# #     df['fuzz_partial_token_set_ratio_Normal'] = df.apply(lambda x: fuzz.partial_token_set_ratio(str(x['Tweet']), str(Normal)), axis=1)\n",
    "# #     df['fuzz_partial_token_set_ratio_Defamation'] = df.apply(lambda x: fuzz.partial_token_set_ratio(str(x['Tweet']), str(Defamation)), axis=1)\n",
    "# #     df['fuzz_partial_token_set_ratio_Terrorism'] = df.apply(lambda x: fuzz.partial_token_set_ratio(str(x['Tweet']), str(Terrorism)), axis=1)\n",
    "    \n",
    "#     df['fuzz_partial_token_sort_ratio_hate_speech'] = df.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['Tweet']), str(x['Tag'])), axis=1)\n",
    "#     df['fuzz_partial_token_sort_ratio_hate_speech'] = df['fuzz_partial_token_sort_ratio_hate_speech'] / 100\n",
    "# #     df['fuzz_partial_token_sort_ratio_Offensive'] = df.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['Tweet']), str(Offensive)), axis=1)\n",
    "# #     df['fuzz_partial_token_sort_ratio_Harassment'] = df.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['Tweet']), str(Harassment)), axis=1)\n",
    "# #     df['fuzz_partial_token_sort_ratio_Normal'] = df.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['Tweet']), str(Normal)), axis=1)\n",
    "# #     df['fuzz_partial_token_sort_ratio_Defamation'] = df.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['Tweet']), str(Defamation)), axis=1)\n",
    "# #     df['fuzz_partial_token_sort_ratio_Terrorism'] = df.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['Tweet']), str(Terrorism)), axis=1)\n",
    "    \n",
    "#     df['fuzz_token_set_ratio_hate_speech'] = df.apply(lambda x: fuzz.token_set_ratio(str(x['Tweet']), str(x['Tag'])), axis=1)\n",
    "#     df['fuzz_token_set_ratio_hate_speech'] = df['fuzz_token_set_ratio_hate_speech'] / 100\n",
    "# #     df['fuzz_token_set_ratio_Offensive'] = df.apply(lambda x: fuzz.token_set_ratio(str(x['Tweet']), str(Offensive)), axis=1)\n",
    "# # #     df['fuzz_token_set_ratio_Harassment'] = df.apply(lambda x: fuzz.token_set_ratio(str(x['Tweet']), str(Harassment)), axis=1)\n",
    "# # #     df['fuzz_token_set_ratio_Normal'] = df.apply(lambda x: fuzz.token_set_ratio(str(x['Tweet']), str(Normal)), axis=1)\n",
    "# # #     df['fuzz_token_set_ratio_Defamation'] = df.apply(lambda x: fuzz.token_set_ratio(str(x['Tweet']), str(Defamation)), axis=1)\n",
    "# # #     df['fuzz_token_set_ratio_Terrorism'] = df.apply(lambda x: fuzz.token_set_ratio(str(x['Tweet']), str(Terrorism)), axis=1)\n",
    "    \n",
    "#     df['fuzz_token_sort_ratio_hate_speech'] = df.apply(lambda x: fuzz.token_sort_ratio(str(x['Tweet']), str(x['Tag'])), axis=1)\n",
    "#     df['fuzz_token_sort_ratio_hate_speech'] = df['fuzz_token_sort_ratio_hate_speech'] / 100\n",
    "# # #     df['fuzz_token_sort_ratio_Offensive'] = df.apply(lambda x: fuzz.token_sort_ratio(str(x['Tweet']), str(Offensive)), axis=1)\n",
    "# #     df['fuzz_token_sort_ratio_Harassment'] = df.apply(lambda x: fuzz.token_sort_ratio(str(x['Tweet']), str(Harassment)), axis=1)\n",
    "# # #     df['fuzz_token_sort_ratio_Normal'] = df.apply(lambda x: fuzz.token_sort_ratio(str(x['Tweet']), str(Normal)), axis=1)\n",
    "# # #     df['fuzz_token_sort_ratio_Defamation'] = df.apply(lambda x: fuzz.token_sort_ratio(str(x['Tweet']), str(Defamation)), axis=1)\n",
    "# # #     df['fuzz_token_sort_ratio_Terrorism'] = df.apply(lambda x: fuzz.token_sort_ratio(str(x['Tweet']), str(Terrorism)), axis=1)\n",
    "\n",
    "#     return df\n",
    "# df = extract_features(data_frame)\n",
    "# # print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WbEs0AG2Pvwf"
   },
   "outputs": [],
   "source": [
    "# # feature_columns = df.columns.drop(['Score','Tag'])\n",
    "\n",
    "# X_normalized = normalize(df[feature_columns].toarray(), norm='l2',axis=1, copy=True, return_norm=False)\n",
    "# # print (X_normalized)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X_normalized, df['Tag'], random_state = 1,test_size=0.2)\n",
    "\n",
    "# result_cols = [\"Classifier\", \"Accuracy\"]\n",
    "# result_frame = pd.DataFrame(columns=result_cols)\n",
    "\n",
    "\n",
    "\n",
    "# feature_columns = df.columns.drop(['Score','Tag'])\n",
    "\n",
    "X_normalized = normalize(bow_matrix, norm='l2', axis=1, copy=True, return_norm=False)\n",
    "# print(X_normalized)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_normalized, df['Tag'], random_state=1, test_size=0.2)\n",
    "\n",
    "result_cols = [\"Classifier\", \"Accuracy\"]\n",
    "result_frame = pd.DataFrame(columns=result_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4HO-ORrfdu5h"
   },
   "outputs": [],
   "source": [
    "# # feature_columns = df.columns.drop(['Score','Tag'])\n",
    "\n",
    "# X_normalized = normalize(df[feature_columns].toarray(), norm='l2',axis=1, copy=True, return_norm=False)\n",
    "# # print (X_normalized)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X_normalized, df['Tag'], random_state = 1,test_size=0.2)\n",
    "\n",
    "# result_cols = [\"Classifier\", \"Accuracy\"]\n",
    "# result_frame = pd.DataFrame(columns=result_cols)\n",
    "\n",
    "\n",
    "\n",
    "# feature_columns = df.columns.drop(['Score','Tag'])\n",
    "\n",
    "X_normalized = normalize(df[feature_columns], norm='l2', axis=1, copy=True, return_norm=False)\n",
    "# print(X_normalized)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_normalized, df['Tag'], random_state=1, test_size=0.2)\n",
    "\n",
    "result_cols = [\"Classifier\", \"Accuracy\"]\n",
    "result_frame = pd.DataFrame(columns=result_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B716Aq3yX4cR"
   },
   "outputs": [],
   "source": [
    "# sns.set_color_codes(\"muted\")\n",
    "# sns.barplot(x='Accuracy', y='Classifier', data=result_frame, color=\"r\")\n",
    "\n",
    "# plt.xlabel('Accuracy %')\n",
    "# plt.title('Classifier Accuracy')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxAHBVV6gB9R"
   },
   "outputs": [],
   "source": [
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-zOH5T_z4t_"
   },
   "outputs": [],
   "source": [
    "############################################"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
